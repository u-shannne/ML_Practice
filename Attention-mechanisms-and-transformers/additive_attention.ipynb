{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加性注意力例子\n",
    "\n",
    "加性注意力分数计算：\n",
    "\n",
    "$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$\n",
    "\n",
    "where learnable parameters\n",
    "\n",
    "$\\mathbf W_q\\in\\mathbb R^{h\\times q}$, $\\mathbf W_k\\in\\mathbb R^{h\\times k}$, and $\\mathbf w_v\\in\\mathbb R^{h}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from lib.d2l_torch import masked_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加性注意力例子\n",
    "\n",
    "-  batch size 为 2，对于每一个 batch，有：\n",
    "- - 两个 `query`，每个的长度为 $20$\n",
    "- 有 $10$ 个 `key-value pair`\n",
    "- - 其中 `key` 的长度为 $2$；\n",
    "- - 其中 `value` 的长度为 $4$；\n",
    "\n",
    "假设 `h` 的大小为 $16$，也就是都转换为长度为 $16$ 的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 20]) torch.Size([2, 10, 2]) torch.Size([2, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "queries, keys = torch.normal(0, 1, (2, 2, 20)), torch.ones((2, 10, 2))\n",
    "# The two value matrices in the values minibatch are identical\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
    "# 打印大小, 第一位都是 batch size\n",
    "print(queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/221019954/anaconda3/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 16]) torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# 分别计算 Wq, Wk, 转换为长度为 h 的向量\n",
    "num_hiddens = 16\n",
    "W_k = nn.LazyLinear(num_hiddens, bias=False)\n",
    "W_q = nn.LazyLinear(num_hiddens, bias=False)\n",
    "\n",
    "_keys, _queries, = W_k(keys), W_q(queries)\n",
    "print(_queries.shape, _keys.shape) # 打印 query 和 key 的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# 计算 Wq + Wk_i, 一个 q 要和所有 k 现加\n",
    "# shape of queries: (batch_size, no. of queries, num_hiddens) --> (batch_size, no. of queries, 1, num_hiddens)\n",
    "# shape of keys: (batch_size, no. of key-value pairs, num_hiddens).  --> (batch_size, 1, no. of key-value pairs, num_hiddens). \n",
    "wq_wk = _queries.unsqueeze(2) +  _keys.unsqueeze(1)\n",
    "print(wq_wk.shape)  # 一共有两个 query, 每个 query 和 10 个 key 得到一个长度为 16 的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# 计算 tanh(w), 这里大小不变\n",
    "features = torch.tanh(wq_wk)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Squeeze torch.Size([2, 2, 10, 1])\n",
      "After Squeeze torch.Size([2, 2, 10])\n",
      "torch.Size([10])\n",
      "tensor([0.3273, 0.3273, 0.3273, 0.3273, 0.3273, 0.3273, 0.3273, 0.3273, 0.3273,\n",
      "        0.3273], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 计算 w_v * tanh(wq + wk) 的值, 一个 query 和 key 得到一个值\n",
    "w_v = nn.LazyLinear(1, bias=False)\n",
    "scores = w_v(features)\n",
    "print('Before Squeeze', scores.shape)\n",
    "scores.squeeze_(-1)\n",
    "print('After Squeeze', scores.shape)\n",
    "print(scores[0][0].shape) # 一个 query 和所有 key 的值, 形状\n",
    "print(scores[0][0]) # 一个 query 和所有 key 的值, 数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 10])\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对数值进行归一化\n",
    "attention_weight = F.softmax(scores, dim=2)\n",
    "print(attention_weight.shape)\n",
    "print(attention_weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 10])\n",
      "tensor([0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 利用 attention mask, 有一些是不会被包含在内的\n",
    "valid_lens = torch.tensor([2, 6]) # 第一个查看前 2 个, 第二个查看前 6 个\n",
    "attention_weight = masked_softmax(scores, valid_lens=valid_lens)\n",
    "\n",
    "print(attention_weight.shape)\n",
    "print(attention_weight[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义「加性注意力」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Additive attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=False) # k --> h\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=False) # q --> h\n",
    "        self.w_v = nn.LazyLinear(1, bias=False) # h --> 1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # 注意这里维度的变化, 会有四个维度\n",
    "        # valid_lens, 考虑多少个 key-value pair\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        # After dimension expansion, \n",
    "        # shape of queries: (batch_size, no. of queries, 1, num_hiddens)\n",
    "        # shape of keys: (batch_size, 1, no. of key-value pairs, num_hiddens). \n",
    "        # Sum them up with broadcasting\n",
    "        # 最终结果的维度是, (batch_size, no. of queries, no. of key-value pairs, num_hiddens)\n",
    "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
    "        features = torch.tanh(features)\n",
    "        # There is only one output of self.w_v, so we remove the last\n",
    "        # one-dimensional entry from the shape. \n",
    "        # Shape of scores: (batch_size, no. of queries, no. of key-value pairs)\n",
    "        # 对每一个 query, 都有 key-value pair 的大小\n",
    "        scores = self.w_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有 1 个 query, query 的长度是 20\n",
    "# 有 10 个 key, key 的长度是 2\n",
    "# 有 10 个 value, value 的长度是 4\n",
    "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
    "# The two value matrices in the values minibatch are identical\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6]) # 第一个查看前 2 个, 第二个查看前 6 个\n",
    "\n",
    "attention = AdditiveAttention(num_hiddens=8, dropout=0.1)\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19098450bf15c53d098daaa826d9da088c802e1c794f68cf2c07b6612df9a31b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
