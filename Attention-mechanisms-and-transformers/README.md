<!--
 * @Author: WANG Maonan
 * @Date: 2022-09-26 11:40:40
 * @Description: 注意力机制介绍
 * @LastEditTime: 2023-01-25 02:14:25
-->
# 文件介绍

## 注意力机制介绍

- 非参数注意力池化
- - nonparametric_attention_pooling.ipynb
- - nonparametric_attention_pooling.py
- 参数化注意力机制
- - parametric_attention_pooling.ipynb
- - parametric_attention_pooling.py

## 注意力分数

- 加性注意力（Additive Attention）
- - additive_attention.ipynb
- - additive_attention.py
- 内积注意力（Scaled Dot-Product Attention）
- - scaled_dot_attention.ipynb
- - scaled_dot_attention.py

## 使用注意力机制的seq2seq

- seq2seq_attention_example.py, 使用随机数据测试;
- seq2seq_attention_data.py, 使用数据集测试, 可以使用这个文件来查看具体 decoder 中 attention 是如何做的.
- seq2seq_attention.ipynb

## 多头注意力

- multihead_attention.py
- multihead_attention.ipynb

## 自注意力

- selfAttention_and_positionEncoding.ipynb

## Transformer 结构

- transformer.py
- transformer.ipynb


